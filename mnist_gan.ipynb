{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YueyingTIAN/minst-GAN/blob/main/mnist_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG4Ii5D7Wpy_"
      },
      "source": [
        "!mkdir input\n",
        "!mkdir outputs"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqKLmvRJVnc3",
        "outputId": "d4b97cf7-6649-4ff6-db08-8e5b8e760f3f"
      },
      "source": [
        "%%writefile vanilla_gan.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import imageio\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "\n",
        "from torchvision.utils import make_grid, save_image\n",
        "from torch.utils.data import DataLoader\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "matplotlib.style.use('ggplot')\n",
        "\n",
        "# learning parameters\n",
        "batch_size = 512\n",
        "epochs = 200\n",
        "sample_size = 64 # fixed sample size\n",
        "nz = 128 # latent vector size\n",
        "k = 1 # number of steps to apply to the discriminator\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "transform = transforms.Compose([\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,),(0.5,)),\n",
        "])\n",
        "\n",
        "to_pil_image = transforms.ToPILImage()\n",
        "\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root='input/data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, nz):\n",
        "        super(Generator, self).__init__()\n",
        "        self.nz = nz\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(self.nz, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Linear(256, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Linear(1024, 784),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x).view(-1, 1, 28, 28)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.n_input = 784\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(self.n_input, 1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        return self.main(x)\n",
        "\n",
        "generator = Generator(nz).to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "print('##### GENERATOR #####')\n",
        "print(generator)\n",
        "print('######################')\n",
        "\n",
        "print('\\n##### DISCRIMINATOR #####')\n",
        "print(discriminator)\n",
        "print('######################')\n",
        "\n",
        "# optimizers\n",
        "optim_g = optim.Adam(generator.parameters(), lr=0.0002)\n",
        "optim_d = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
        "\n",
        "# loss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "losses_g = [] # to store generator loss after each epoch\n",
        "losses_d = [] # to store discriminator loss after each epoch\n",
        "images = [] # to store images generatd by the generator\n",
        "\n",
        "# to create real labels (1s)\n",
        "def label_real(size):\n",
        "    data = torch.ones(size, 1)\n",
        "    return data.to(device)\n",
        "\n",
        "# to create fake labels (0s)\n",
        "def label_fake(size):\n",
        "    data = torch.zeros(size, 1)\n",
        "    return data.to(device)\n",
        "\n",
        "# function to create the noise vector\n",
        "def create_noise(sample_size, nz):\n",
        "    return torch.randn(sample_size, nz).to(device)\n",
        "\n",
        "# to save the images generated by the generator\n",
        "def save_generator_image(image, path):\n",
        "    save_image(image, path)\n",
        "\n",
        "# function to train the discriminator network\n",
        "def train_discriminator(optimizer, data_real, data_fake):\n",
        "    b_size = data_real.size(0)\n",
        "    real_label = label_real(b_size)\n",
        "    fake_label = label_fake(b_size)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output_real = discriminator(data_real)\n",
        "    loss_real = criterion(output_real, real_label)\n",
        "\n",
        "    output_fake = discriminator(data_fake)\n",
        "    loss_fake = criterion(output_fake, fake_label)\n",
        "\n",
        "\n",
        "    loss_real.backward()\n",
        "    loss_fake.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss_real + loss_fake\n",
        "\n",
        "# function to train the generator network\n",
        "def train_generator(optimizer, data_fake):\n",
        "    b_size = data_fake.size(0)\n",
        "    real_label = label_real(b_size)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = discriminator(data_fake)\n",
        "    loss = criterion(output, real_label)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss    \n",
        "\n",
        "# create the noise vector\n",
        "noise = create_noise(sample_size, nz)\n",
        "\n",
        "generator.train()\n",
        "discriminator.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    loss_g = 0.0\n",
        "    loss_d = 0.0\n",
        "    for bi, data in tqdm(enumerate(train_loader), total=int(len(train_data)/train_loader.batch_size)):\n",
        "        image, _ = data\n",
        "        image = image.to(device)\n",
        "        b_size = len(image)\n",
        "        # run the discriminator for k number of steps\n",
        "        for step in range(k):\n",
        "            data_fake = generator(create_noise(b_size, nz)).detach()\n",
        "            data_real = image\n",
        "            # train the discriminator network\n",
        "            loss_d += train_discriminator(optim_d, data_real, data_fake)\n",
        "        data_fake = generator(create_noise(b_size, nz))\n",
        "        # train the generator network\n",
        "        loss_g += train_generator(optim_g, data_fake)\n",
        "\n",
        "    # create the final fake image for the epoch\n",
        "    generated_img = generator(noise).cpu().detach()\n",
        "    # make the images as grid\n",
        "    generated_img = make_grid(generated_img)\n",
        "    # save the generated torch tensor models to disk\n",
        "    save_generator_image(generated_img, f\"outputs/gen_img{epoch}.png\")\n",
        "    images.append(generated_img)\n",
        "    epoch_loss_g = loss_g / bi # total generator loss for the epoch\n",
        "    epoch_loss_d = loss_d / bi # total discriminator loss for the epoch\n",
        "    losses_g.append(epoch_loss_g)\n",
        "    losses_d.append(epoch_loss_d)\n",
        "    \n",
        "    print(f\"Epoch {epoch} of {epochs}\")\n",
        "    print(f\"Generator loss: {epoch_loss_g:.8f}, Discriminator loss: {epoch_loss_d:.8f}\")\n",
        "\n",
        "print('DONE TRAINING')\n",
        "torch.save(generator.state_dict(), 'outputs/generator.pth')\n",
        "\n",
        "# save the generated images as GIF file\n",
        "imgs = [np.array(to_pil_image(img)) for img in images]\n",
        "imageio.mimsave('outputs/generator_images.gif', imgs)\n",
        "\n",
        "# plot and save the generator and discriminator loss\n",
        "plt.figure()\n",
        "plt.plot(losses_g, label='Generator loss')\n",
        "plt.plot(losses_d, label='Discriminator Loss')\n",
        "plt.legend()\n",
        "plt.savefig('outputs/loss.png')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing vanilla_gan.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EXbGUjNue_oC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sqfX9GQOe_Y3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O1X4QED6e_HY"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DIUlKRUde-vi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Du60i3R5gJCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yNjchFaUgJZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ojSHxARWzRR",
        "outputId": "c865ab0e-7878-4629-def3-c4cfeabf34f9"
      },
      "source": [
        "!python vanilla_gan.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to input/data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n",
            "26427392it [00:01, 16791875.87it/s]                 \n",
            "Extracting input/data/FashionMNIST/raw/train-images-idx3-ubyte.gz to input/data/FashionMNIST/raw\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to input/data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n",
            "32768it [00:00, 111664.00it/s]\n",
            "Extracting input/data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to input/data/FashionMNIST/raw\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to input/data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "4423680it [00:00, 4816276.87it/s]                \n",
            "Extracting input/data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to input/data/FashionMNIST/raw\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to input/data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "8192it [00:00, 39341.88it/s]\n",
            "Extracting input/data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to input/data/FashionMNIST/raw\n",
            "Processing...\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
            "Done!\n",
            "##### GENERATOR #####\n",
            "Generator(\n",
            "  (main): Sequential(\n",
            "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
            "    (1): LeakyReLU(negative_slope=0.2)\n",
            "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
            "    (3): LeakyReLU(negative_slope=0.2)\n",
            "    (4): Linear(in_features=512, out_features=1024, bias=True)\n",
            "    (5): LeakyReLU(negative_slope=0.2)\n",
            "    (6): Linear(in_features=1024, out_features=784, bias=True)\n",
            "    (7): Tanh()\n",
            "  )\n",
            ")\n",
            "######################\n",
            "\n",
            "##### DISCRIMINATOR #####\n",
            "Discriminator(\n",
            "  (main): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "    (1): LeakyReLU(negative_slope=0.2)\n",
            "    (2): Dropout(p=0.3, inplace=False)\n",
            "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2)\n",
            "    (5): Dropout(p=0.3, inplace=False)\n",
            "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2)\n",
            "    (8): Dropout(p=0.3, inplace=False)\n",
            "    (9): Linear(in_features=256, out_features=1, bias=True)\n",
            "    (10): Sigmoid()\n",
            "  )\n",
            ")\n",
            "######################\n",
            "118it [00:12,  9.73it/s]             \n",
            "Epoch 0 of 200\n",
            "Generator loss: 1.28380990, Discriminator loss: 1.00167513\n",
            "118it [00:11,  9.88it/s]             \n",
            "Epoch 1 of 200\n",
            "Generator loss: 1.54958630, Discriminator loss: 0.70626169\n",
            "118it [00:11,  9.84it/s]\n",
            "Epoch 2 of 200\n",
            "Generator loss: 4.01227856, Discriminator loss: 0.50264722\n",
            "118it [00:11,  9.85it/s]             \n",
            "Epoch 3 of 200\n",
            "Generator loss: 3.88136625, Discriminator loss: 0.82140148\n",
            "118it [00:11,  9.84it/s]             \n",
            "Epoch 4 of 200\n",
            "Generator loss: 2.89859581, Discriminator loss: 0.38353422\n",
            "118it [00:12,  9.78it/s]\n",
            "Epoch 5 of 200\n",
            "Generator loss: 4.69612646, Discriminator loss: 0.45601350\n",
            "118it [00:11,  9.90it/s]             \n",
            "Epoch 6 of 200\n",
            "Generator loss: 3.48392582, Discriminator loss: 0.51368976\n",
            "118it [00:11,  9.91it/s]\n",
            "Epoch 7 of 200\n",
            "Generator loss: 3.58306527, Discriminator loss: 0.52515101\n",
            "118it [00:11,  9.90it/s]\n",
            "Epoch 8 of 200\n",
            "Generator loss: 3.02182436, Discriminator loss: 0.81383133\n",
            "118it [00:12,  9.82it/s]             \n",
            "Epoch 9 of 200\n",
            "Generator loss: 2.75331473, Discriminator loss: 0.69407350\n",
            " 82% 96/117 [00:09<00:02,  9.87it/s]\n",
            " 82% 96/117 [00:09<00:02,  9.79it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYJgc9K4W03I"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}